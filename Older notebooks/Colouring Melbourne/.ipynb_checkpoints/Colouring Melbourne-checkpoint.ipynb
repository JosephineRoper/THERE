{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec4193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import sys\n",
    "# adding functions \n",
    "sys.path.insert(0, 'C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Open-Walk-Index')\n",
    "from walkability_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40338e69",
   "metadata": {},
   "source": [
    "Choose a projected CRS to be used for all distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28571a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_crs = \"EPSG:7855\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865078a9",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Data sources:\n",
    "1. Shape of Greater Melbourne - used to clip points if not already clipped to the city\n",
    "2. Points of interest from OSM\n",
    "3. PTV public transport stops\n",
    "4. Additional POIs from VicMaps Features of Interest collection\n",
    "5. Employment data - processed from ABS originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Colouring data & results\\\\Melbourne Data\\\\\"\n",
    "data = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Data\\\\\"\n",
    "Greater_Melbourne = gpd.read_file((folder + \n",
    "                                   \"Greater_Melbourne_GCCSA_2016.shp\")\n",
    "                                 ).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c494196",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_poi_points = gpd.read_file(''.join((data, \n",
    "    \"OSM-australia-latest-free\\\\gis_osm_pois_free_1.shp\")))\n",
    "osm_poi_areas = gpd.read_file(data + \n",
    "    \"OSM-australia-latest-free\\\\gis_osm_pois_a_free_1.shp\")\n",
    "osm_transport_points = gpd.read_file(data +\n",
    "    \"OSM-australia-latest-free\\\\gis_osm_transport_free_1.shp\")\n",
    "osm_transport_areas =  gpd.read_file(data +\n",
    "    \"OSM-australia-latest-free\\\\gis_osm_transport_a_free_1.shp\")\n",
    "osm_parks_vertices = gpd.read_file(''.join((data, \n",
    "    \"OSM-australia-latest-free\\\\OSM parks vertices.gpkg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48e4c3",
   "metadata": {},
   "source": [
    "Convert polygonal datasets to points and any multipart datasets to single part. Clip OSM data to Greater Melbourne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ebdc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_pois_2 = single_points(osm_poi_areas)\n",
    "osm_transport_2 = single_points(osm_transport_areas)\n",
    "osm_parks_vertices = single_points(osm_parks_vertices)\n",
    "\n",
    "osm_df = pd.concat([osm_poi_points, osm_pois_2, osm_transport_points, \n",
    "                    osm_transport_2, osm_parks_vertices]).to_crs(proj_crs)\n",
    "\n",
    "osm_df = gpd.clip(osm_df, Greater_Melbourne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1b076",
   "metadata": {},
   "source": [
    "Import PTV data - already clipped to Greater Melbourne area, VicMaps data, and employment data (prepared using Employment points.ipnyb and ABS data).\n",
    "I have added specific service type columns at this point, even though they will be rolled into 'train' and 'other' for now, to make it easier to change the categories later if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a68580",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = gpd.read_file(folder + \"PTV data\\\\PTV_METRO_BUS_STOP.SHP\").assign(fclass='bus')\n",
    "regional_bus = gpd.read_file(folder + \"PTV data\\\\PTV_REGIONAL_BUS_STOP.SHP\").assign(fclass='regional_bus')\n",
    "tram = gpd.read_file(folder + \"PTV data\\\\PTV_METRO_TRAM_STOP.SHP\").assign(fclass='tram')\n",
    "coach = gpd.read_file(folder + \"PTV data\\\\PTV_REGIONAL_COACH_STOP.SHP\").assign(fclass='coach')\n",
    "train = gpd.read_file(folder + \"PTV data\\\\PTV_REGIONAL_TRAIN_STATION.SHP\").assign(fclass='regional_train')\n",
    "regional_train = gpd.read_file(folder + \"PTV data\\\\PTV_METRO_TRAIN_STATION.SHP\").assign(fclass='train')\n",
    "\n",
    "PTV = pd.concat([bus, regional_bus, tram, coach, train, regional_train]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25120e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vicmaps_points = gpd.read_file(folder + \"VicMap Features of Interest\\\\FOI_POINT.shp\")\n",
    "vicmaps_areas = gpd.read_file(folder + \"VicMap Features of Interest\\\\VMFOI.gdb\")\n",
    "\n",
    "vicmaps = pd.concat([vicmaps_points, vicmaps_areas]).to_crs(proj_crs)\n",
    "\n",
    "vicmaps = gpd.clip(vicmaps, Greater_Melbourne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d8bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = gpd.read_file(folder + \"Vic_Employment_meshblocks.gpkg\").to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31928c3",
   "metadata": {},
   "source": [
    "### Categorise and weight POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70870",
   "metadata": {},
   "source": [
    "Categorise POI data - change classes depending on your analysis and your data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588f14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "metro_categories = {'transport':['bus', 'regional_bus', 'tram'], \n",
    "                    'trains':['train', 'regional_train', 'coach']}\n",
    "\n",
    "metro_categorised = categorise_pois(PTV, metro_categories, \n",
    "                                 category_column='fclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d17999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "['toilet' 'bench' 'drinking_water' 'shelter' 'camp_site' 'monument'\n",
      " 'memorial' 'fire_station' 'telephone' 'tourist_info' 'hunting_stand'\n",
      " 'camera_surveillance' 'waste_basket' 'motel' 'caravan_site' 'graveyard'\n",
      " 'fountain' 'guesthouse' 'water_tower' 'tower' 'police' 'public_building'\n",
      " 'vending_any' 'hotel' 'nursing_home' 'comms_tower' 'vending_parking'\n",
      " 'recycling' 'lighthouse' 'wastewater_plant' 'bicycle_rental'\n",
      " 'bed_and_breakfast' 'courthouse' 'town_hall' 'car_rental'\n",
      " 'vending_machine' 'taxi' 'hostel' 'water_well' 'water_works'\n",
      " 'recycling_clothes' 'recycling_glass' 'chalet' 'prison' 'embassy'\n",
      " 'recycling_paper' 'alpine_hut']\n"
     ]
    }
   ],
   "source": [
    "osm_categories = {\"eating\" : ['restaurant', 'pub', 'cafe', 'fast_food', \n",
    "                              'food_court', 'bakery', 'bar', 'nightclub', 'biergarten'], \n",
    "                  'groceries' : ['supermarket', 'chemist', 'pharmacy', 'greengrocer', \n",
    "                                 'convenience', 'butcher', 'beverages', 'alcohol'], \n",
    "                  'shopping' : ['mall', 'bicycle_shop', 'clothes', \n",
    "                                'department_store', 'doityourself', \n",
    "                                'outdoor_shop', 'stationery', 'bookshop', \n",
    "                                'gift_shop', 'newsagent', 'car_dealership', \n",
    "                                'kiosk', 'furniture_shop', 'sports_shop', \n",
    "                                'garden_centre', 'computer_shop', 'shoe_shop', \n",
    "                                'beauty_shop', 'florist', 'video_shop', 'toy_shop', \n",
    "                                'mobile_phone_shop', 'jeweller', 'travel_agent'], \n",
    "                  'errands' : ['post_box', 'post_office', 'bank', 'atm',\n",
    "                               'doctors', 'dentist', 'laundry', 'hospital',\n",
    "                               'car_wash', 'veterinary', 'hairdresser', 'optician'], \n",
    "                  'parks' : ['viewpoint', 'park', 'playground', 'picnic_site', \n",
    "                             'pitch', 'swimming_pool', 'sports_centre', \n",
    "                             'golf_course', 'track', 'dog_park'], \n",
    "                  'education' : ['college', 'school', 'kindergarten', 'university'], \n",
    "                  'entertainment' : ['library', 'attraction', 'stadium', \n",
    "                                     'arts_centre', 'theatre', 'artwork', \n",
    "                                     'archaeological', 'cinema', 'museum', \n",
    "                                     'ruins', 'observation_tower', \n",
    "                                     'community_centre', 'zoo', 'castle', \n",
    "                                     'theme_park', 'ice_rink'], \n",
    "                 'trains' : ['ferry_terminal', 'railway_station', 'bus_station', \n",
    "                             'tram_stop', 'railway_halt', 'publictransport'], \n",
    "                 'transport' : ['car_sharing', 'bus_stop']}\n",
    "\n",
    "osm_categorised = categorise_pois(osm_df, osm_categories, \n",
    "                                  category_column='fclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040452bb",
   "metadata": {},
   "source": [
    "In the VicMaps data, the 'community space' collection is caravan parks, camping areas (generally outside Greater Melbourne) and rest areas, not considered relevant. The 'community venue' collection is community centres, halls, senior clubs, scouts etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9a148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "['sign' 'landmark' 'care facility' 'emergency facility'\n",
      " 'communication service' nan 'community space' 'admin facility'\n",
      " 'dumping ground' 'control point' 'excavation site' 'place'\n",
      " 'storage facility' 'pipeline facility' 'defence site']\n"
     ]
    }
   ],
   "source": [
    "vicmaps_categories = {\"eating\" : [], \n",
    "                  'groceries' : [], \n",
    "                  'shopping' : [], \n",
    "                  'errands' : ['hospital', 'health facility', 'place of worship'], \n",
    "                  'parks' : ['recreational resource', 'reserve','sport facility'], \n",
    "                  'education' : ['education centre'], \n",
    "                  'entertainment' : ['cultural centre', 'commercial facility', 'community venue'], \n",
    "                 'trains' : [], \n",
    "                 'transport' : []}\n",
    "\n",
    "\n",
    "vicmaps_categorised = categorise_pois(vicmaps, vicmaps_categories, \n",
    "                                  category_column='FTYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1bf6",
   "metadata": {},
   "source": [
    "Need to remove potential overlap between different data sources (and inside some data sources). For this dataset it's around 30% because there is overlap of public transport stops between OSM and transport agencies, and overlap of places like parks and schools between OSM and VicMaps. Then take this combined POI set and clip it to the study area: should be the same area as is covered by the network. This is important otherwise points outside the network may be erroneously linked to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747397b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 35.67% duplicate points from dataframes\n"
     ]
    }
   ],
   "source": [
    "pois = remove_duplicate_pois([osm_categorised, vicmaps_categorised,\n",
    "                              metro_categorised], buffer=10)\n",
    "\n",
    "pois = gpd.clip(pois, Greater_Melbourne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ca191",
   "metadata": {},
   "source": [
    "Choose walk index weightings, and output the sums of each category and the total to check. The walk index will be out of 100 regardless of this sum, but it is important to note that eg. shopping is only '10% of the walk index' if shopping is 10 out of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd0dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_weights = {\n",
    "    \"employment\": [10],\n",
    "    \"eating\": [3, 3, 3, 2, 2, 1, 1, 1, 1, 1],\n",
    "    \"groceries\": [10, 4],\n",
    "    \"shopping\": [2, 2, 2, 2, 2],\n",
    "    \"errands\": [6, 2, 4],\n",
    "    \"parks\": [6],\n",
    "    \"education\": [10],\n",
    "    \"entertainment\": [5],\n",
    "    \"trains\": [10],\n",
    "    \"transport\": [2.5, 2.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2440fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'employment': 10, 'eating': 18, 'groceries': 14, 'shopping': 10, 'errands': 12, 'parks': 6, 'education': 10, 'entertainment': 5, 'trains': 10, 'transport': 5.0}\n",
      "total:  100.0\n"
     ]
    }
   ],
   "source": [
    "category_sums = {k: sum(v) for k, v in poi_weights.items()}\n",
    "total = sum(category_sums.values())\n",
    "print(category_sums)\n",
    "print(\"total: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd7581",
   "metadata": {},
   "source": [
    "### Import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b6f48",
   "metadata": {},
   "source": [
    "In this case the network is already in the same projected CRS as everything else but I have left in the transformation to be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d529acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (3,4,7,8,9,10,11,16,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (6,13,14,16,17,18,19,21,22,28,29,31,36,37) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# reading directly with geopandas.read_file crashes on my computer so I read into pandas then convert to gdf instead\n",
    "edges_df = pd.read_csv(folder + \"melbourne_edges.csv\")\n",
    "nodes_df = pd.read_csv(folder + \"melbourne_nodes.csv\")\n",
    "edges = gpd.GeoDataFrame(edges_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(edges_df['geometry'])).set_crs(\"EPSG:7856\")\n",
    "nodes = gpd.GeoDataFrame(nodes_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(nodes_df['geometry'])).set_crs(\"EPSG:7856\")\n",
    "edges = edges.to_crs(proj_crs)\n",
    "nodes = nodes.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395d9f",
   "metadata": {},
   "source": [
    "Pandana expects edges to have a two item index based on the same IDs as the node index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31dda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes.set_index('connect_id',inplace=True)   #this is already the case this time for some reason\n",
    "\n",
    "edges['from_idx'] = edges['from']\n",
    "edges['to_idx'] = edges['to']\n",
    "edges= edges.set_index(['from_idx', 'to_idx'])\n",
    "edges.index.names= ['from_idx','to_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95c0746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges[edges['to'].isin(nodes['Unnamed: 0']) & edges['from'].isin(nodes['Unnamed: 0'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea582800",
   "metadata": {},
   "source": [
    "Pandana network creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1149fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_network = pdna.Network(nodes['geometry'].x, nodes['geometry'].y,\n",
    "                                   edges['from'], edges['to'], \n",
    "                                   edges[['length']])\n",
    "\n",
    "maximum_dist = 2400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4efc6",
   "metadata": {},
   "source": [
    "Pandana network querying. The 'employment' category is empty because we didn't add the employment points to the POI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63e13f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category employment is empty\n",
      "Finished category: eating\n",
      "Finished category: groceries\n",
      "Finished category: shopping\n",
      "Finished category: errands\n",
      "Finished category: parks\n",
      "Finished category: education\n",
      "Finished category: entertainment\n",
      "Finished category: trains\n",
      "Finished category: transport\n"
     ]
    }
   ],
   "source": [
    "results = walk_index(distance_network, pois, poi_weights, distance=maximum_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ca93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Melbourne_colournoemployment_220222.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f521902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x                  3.204843e+05\n",
       "y                  5.859578e+06\n",
       "employment_10      0.000000e+00\n",
       "eating1            2.400000e+03\n",
       "eating2            2.400000e+03\n",
       "eating3            2.400000e+03\n",
       "eating4            2.400000e+03\n",
       "eating5            2.400000e+03\n",
       "eating6            2.400000e+03\n",
       "eating7            2.400000e+03\n",
       "eating8            2.400000e+03\n",
       "eating9            2.400000e+03\n",
       "eating10           2.400000e+03\n",
       "eating_18          0.000000e+00\n",
       "groceries1         2.400000e+03\n",
       "groceries2         2.400000e+03\n",
       "groceries_14       0.000000e+00\n",
       "shopping1          2.400000e+03\n",
       "shopping2          2.400000e+03\n",
       "shopping3          2.400000e+03\n",
       "shopping4          2.400000e+03\n",
       "shopping5          2.400000e+03\n",
       "shopping_10        0.000000e+00\n",
       "errands1           2.400000e+03\n",
       "errands2           2.400000e+03\n",
       "errands3           2.400000e+03\n",
       "errands_12         0.000000e+00\n",
       "parks1             0.000000e+00\n",
       "parks_6            6.000000e+00\n",
       "education1         2.400000e+03\n",
       "education_10       0.000000e+00\n",
       "entertainment1     2.400000e+03\n",
       "entertainment_5    0.000000e+00\n",
       "trains1            3.551810e+02\n",
       "trains_10          7.010465e+00\n",
       "transport1         3.551810e+02\n",
       "transport2         1.467515e+03\n",
       "transport_5        2.328860e+00\n",
       "Walk_Index         1.533933e+01\n",
       "jobs               0.000000e+00\n",
       "employment         0.000000e+00\n",
       "Name: 4831719, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.iloc[4831719]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "613b6abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>employment_10</th>\n",
       "      <th>eating1</th>\n",
       "      <th>eating2</th>\n",
       "      <th>eating3</th>\n",
       "      <th>eating4</th>\n",
       "      <th>eating5</th>\n",
       "      <th>eating6</th>\n",
       "      <th>eating7</th>\n",
       "      <th>...</th>\n",
       "      <th>entertainment1</th>\n",
       "      <th>entertainment_5</th>\n",
       "      <th>trains1</th>\n",
       "      <th>trains_10</th>\n",
       "      <th>transport1</th>\n",
       "      <th>transport2</th>\n",
       "      <th>transport_5</th>\n",
       "      <th>Walk_Index</th>\n",
       "      <th>jobs</th>\n",
       "      <th>employment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1460246</th>\n",
       "      <td>268074.594051</td>\n",
       "      <td>5.829563e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>655.931030</td>\n",
       "      <td>688.497009</td>\n",
       "      <td>835.465027</td>\n",
       "      <td>1199.468018</td>\n",
       "      <td>1292.531006</td>\n",
       "      <td>1471.769043</td>\n",
       "      <td>1508.770996</td>\n",
       "      <td>...</td>\n",
       "      <td>1693.038940</td>\n",
       "      <td>0.919798</td>\n",
       "      <td>1508.770996</td>\n",
       "      <td>2.211816</td>\n",
       "      <td>281.856995</td>\n",
       "      <td>469.033997</td>\n",
       "      <td>3.449970</td>\n",
       "      <td>41.118455</td>\n",
       "      <td>2388.172076</td>\n",
       "      <td>0.341167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460247</th>\n",
       "      <td>271115.911746</td>\n",
       "      <td>5.831284e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>668.406006</td>\n",
       "      <td>700.971985</td>\n",
       "      <td>847.940002</td>\n",
       "      <td>1211.942993</td>\n",
       "      <td>1305.005981</td>\n",
       "      <td>1484.244019</td>\n",
       "      <td>1521.245972</td>\n",
       "      <td>...</td>\n",
       "      <td>1705.514038</td>\n",
       "      <td>0.908395</td>\n",
       "      <td>1521.245972</td>\n",
       "      <td>2.184395</td>\n",
       "      <td>294.332001</td>\n",
       "      <td>481.509003</td>\n",
       "      <td>3.407199</td>\n",
       "      <td>40.608689</td>\n",
       "      <td>2358.564716</td>\n",
       "      <td>0.336938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460248</th>\n",
       "      <td>270185.934382</td>\n",
       "      <td>5.825931e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>652.737976</td>\n",
       "      <td>685.304016</td>\n",
       "      <td>832.271973</td>\n",
       "      <td>1196.275024</td>\n",
       "      <td>1289.338013</td>\n",
       "      <td>1468.576050</td>\n",
       "      <td>1505.578003</td>\n",
       "      <td>...</td>\n",
       "      <td>1689.845947</td>\n",
       "      <td>0.922740</td>\n",
       "      <td>1505.578003</td>\n",
       "      <td>2.218890</td>\n",
       "      <td>278.664001</td>\n",
       "      <td>465.841003</td>\n",
       "      <td>3.461003</td>\n",
       "      <td>41.249956</td>\n",
       "      <td>2395.809685</td>\n",
       "      <td>0.342259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460249</th>\n",
       "      <td>269813.466914</td>\n",
       "      <td>5.829363e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>661.013977</td>\n",
       "      <td>693.580017</td>\n",
       "      <td>840.547974</td>\n",
       "      <td>1204.551025</td>\n",
       "      <td>1297.614014</td>\n",
       "      <td>1476.852051</td>\n",
       "      <td>1513.854004</td>\n",
       "      <td>...</td>\n",
       "      <td>1698.121948</td>\n",
       "      <td>0.915135</td>\n",
       "      <td>1513.854004</td>\n",
       "      <td>2.200602</td>\n",
       "      <td>286.940002</td>\n",
       "      <td>474.117004</td>\n",
       "      <td>3.432478</td>\n",
       "      <td>40.909981</td>\n",
       "      <td>2376.063791</td>\n",
       "      <td>0.339438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460250</th>\n",
       "      <td>272326.247677</td>\n",
       "      <td>5.829353e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>673.601013</td>\n",
       "      <td>706.166992</td>\n",
       "      <td>853.135010</td>\n",
       "      <td>1217.137939</td>\n",
       "      <td>1310.201050</td>\n",
       "      <td>1489.438965</td>\n",
       "      <td>1526.441040</td>\n",
       "      <td>...</td>\n",
       "      <td>1710.708984</td>\n",
       "      <td>0.903688</td>\n",
       "      <td>1526.441040</td>\n",
       "      <td>2.173077</td>\n",
       "      <td>299.527008</td>\n",
       "      <td>486.704010</td>\n",
       "      <td>3.389544</td>\n",
       "      <td>40.398274</td>\n",
       "      <td>2346.343702</td>\n",
       "      <td>0.335192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831717</th>\n",
       "      <td>320473.326838</td>\n",
       "      <td>5.859477e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1665.452026</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1624.034058</td>\n",
       "      <td>1.971020</td>\n",
       "      <td>158.395996</td>\n",
       "      <td>158.395996</td>\n",
       "      <td>4.267559</td>\n",
       "      <td>23.299406</td>\n",
       "      <td>406.329493</td>\n",
       "      <td>0.058047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831718</th>\n",
       "      <td>320378.125054</td>\n",
       "      <td>5.859488e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>104.323997</td>\n",
       "      <td>283.550995</td>\n",
       "      <td>680.067017</td>\n",
       "      <td>751.067993</td>\n",
       "      <td>866.624023</td>\n",
       "      <td>866.624023</td>\n",
       "      <td>897.437012</td>\n",
       "      <td>...</td>\n",
       "      <td>679.750000</td>\n",
       "      <td>2.533718</td>\n",
       "      <td>1353.634033</td>\n",
       "      <td>2.582999</td>\n",
       "      <td>284.338989</td>\n",
       "      <td>346.885010</td>\n",
       "      <td>3.648495</td>\n",
       "      <td>44.755285</td>\n",
       "      <td>791.281534</td>\n",
       "      <td>0.113040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831719</th>\n",
       "      <td>320484.259324</td>\n",
       "      <td>5.859578e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>355.181000</td>\n",
       "      <td>7.010465</td>\n",
       "      <td>355.181000</td>\n",
       "      <td>1467.515015</td>\n",
       "      <td>2.328860</td>\n",
       "      <td>15.339326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831720</th>\n",
       "      <td>320492.045272</td>\n",
       "      <td>5.859622e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>570.848022</td>\n",
       "      <td>570.848022</td>\n",
       "      <td>745.591980</td>\n",
       "      <td>1332.422974</td>\n",
       "      <td>1434.894043</td>\n",
       "      <td>1529.667969</td>\n",
       "      <td>1679.235962</td>\n",
       "      <td>...</td>\n",
       "      <td>1653.838013</td>\n",
       "      <td>0.956571</td>\n",
       "      <td>1875.931030</td>\n",
       "      <td>1.532123</td>\n",
       "      <td>319.570007</td>\n",
       "      <td>462.863007</td>\n",
       "      <td>3.389851</td>\n",
       "      <td>40.956200</td>\n",
       "      <td>2128.813696</td>\n",
       "      <td>0.304116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831721</th>\n",
       "      <td>320366.733417</td>\n",
       "      <td>5.859637e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>489.399994</td>\n",
       "      <td>585.036011</td>\n",
       "      <td>757.778992</td>\n",
       "      <td>841.041016</td>\n",
       "      <td>919.765015</td>\n",
       "      <td>941.315979</td>\n",
       "      <td>985.505981</td>\n",
       "      <td>...</td>\n",
       "      <td>246.572998</td>\n",
       "      <td>3.907372</td>\n",
       "      <td>738.424988</td>\n",
       "      <td>4.778660</td>\n",
       "      <td>239.453003</td>\n",
       "      <td>243.283997</td>\n",
       "      <td>3.927768</td>\n",
       "      <td>47.208176</td>\n",
       "      <td>2487.437681</td>\n",
       "      <td>0.355348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1866261 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     x             y  employment_10      eating1      eating2  \\\n",
       "1460246  268074.594051  5.829563e+06              0   655.931030   688.497009   \n",
       "1460247  271115.911746  5.831284e+06              0   668.406006   700.971985   \n",
       "1460248  270185.934382  5.825931e+06              0   652.737976   685.304016   \n",
       "1460249  269813.466914  5.829363e+06              0   661.013977   693.580017   \n",
       "1460250  272326.247677  5.829353e+06              0   673.601013   706.166992   \n",
       "...                ...           ...            ...          ...          ...   \n",
       "4831717  320473.326838  5.859477e+06              0  1665.452026  2400.000000   \n",
       "4831718  320378.125054  5.859488e+06              0   104.323997   283.550995   \n",
       "4831719  320484.259324  5.859578e+06              0  2400.000000  2400.000000   \n",
       "4831720  320492.045272  5.859622e+06              0   570.848022   570.848022   \n",
       "4831721  320366.733417  5.859637e+06              0   489.399994   585.036011   \n",
       "\n",
       "             eating3      eating4      eating5      eating6      eating7  ...  \\\n",
       "1460246   835.465027  1199.468018  1292.531006  1471.769043  1508.770996  ...   \n",
       "1460247   847.940002  1211.942993  1305.005981  1484.244019  1521.245972  ...   \n",
       "1460248   832.271973  1196.275024  1289.338013  1468.576050  1505.578003  ...   \n",
       "1460249   840.547974  1204.551025  1297.614014  1476.852051  1513.854004  ...   \n",
       "1460250   853.135010  1217.137939  1310.201050  1489.438965  1526.441040  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "4831717  2400.000000  2400.000000  2400.000000  2400.000000  2400.000000  ...   \n",
       "4831718   680.067017   751.067993   866.624023   866.624023   897.437012  ...   \n",
       "4831719  2400.000000  2400.000000  2400.000000  2400.000000  2400.000000  ...   \n",
       "4831720   745.591980  1332.422974  1434.894043  1529.667969  1679.235962  ...   \n",
       "4831721   757.778992   841.041016   919.765015   941.315979   985.505981  ...   \n",
       "\n",
       "         entertainment1  entertainment_5      trains1  trains_10  transport1  \\\n",
       "1460246     1693.038940         0.919798  1508.770996   2.211816  281.856995   \n",
       "1460247     1705.514038         0.908395  1521.245972   2.184395  294.332001   \n",
       "1460248     1689.845947         0.922740  1505.578003   2.218890  278.664001   \n",
       "1460249     1698.121948         0.915135  1513.854004   2.200602  286.940002   \n",
       "1460250     1710.708984         0.903688  1526.441040   2.173077  299.527008   \n",
       "...                 ...              ...          ...        ...         ...   \n",
       "4831717     2400.000000         0.000000  1624.034058   1.971020  158.395996   \n",
       "4831718      679.750000         2.533718  1353.634033   2.582999  284.338989   \n",
       "4831719     2400.000000         0.000000   355.181000   7.010465  355.181000   \n",
       "4831720     1653.838013         0.956571  1875.931030   1.532123  319.570007   \n",
       "4831721      246.572998         3.907372   738.424988   4.778660  239.453003   \n",
       "\n",
       "          transport2  transport_5  Walk_Index         jobs  employment  \n",
       "1460246   469.033997     3.449970   41.118455  2388.172076    0.341167  \n",
       "1460247   481.509003     3.407199   40.608689  2358.564716    0.336938  \n",
       "1460248   465.841003     3.461003   41.249956  2395.809685    0.342259  \n",
       "1460249   474.117004     3.432478   40.909981  2376.063791    0.339438  \n",
       "1460250   486.704010     3.389544   40.398274  2346.343702    0.335192  \n",
       "...              ...          ...         ...          ...         ...  \n",
       "4831717   158.395996     4.267559   23.299406   406.329493    0.058047  \n",
       "4831718   346.885010     3.648495   44.755285   791.281534    0.113040  \n",
       "4831719  1467.515015     2.328860   15.339326     0.000000    0.000000  \n",
       "4831720   462.863007     3.389851   40.956200  2128.813696    0.304116  \n",
       "4831721   243.283997     3.927768   47.208176  2487.437681    0.355348  \n",
       "\n",
       "[1866261 rows x 41 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc063b9",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8af95",
   "metadata": {},
   "source": [
    "The current approach is to find up to 100 closest employment nodes within the maximum distance. Then look up the number of jobs at each one, apply a distance decay function to each distance, multiply these together, and sum.\n",
    "\n",
    "An alternative approach which would be more convenient would be to use the Pandana 'aggregate' function which aggregates from all nodes within the maximum distance. However, there is limited ability to change the distance decay rate within the aggregation function. It can either be flat (no decay), linear (going to 0 at the max distance), or exponential where beta is set as 1/max distance. For walking I would like a beta of 0.001, but this requires the radius to be 1000m. If the radius is 2400m, beta is only 0.0004. This can be changed in the future if the Pandana function is updated to take a decay parameter. The aggregate function also seems to be slower than expected with the kind of network & distances I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2392d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = single_points(employment_centrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "723d9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = employment_centrs.droplevel(1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23519014",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs=employment_centrs.set_index('Jobs')\n",
    "\n",
    "x, y = (employment_centrs['geometry'].x, employment_centrs['geometry'].y)\n",
    "\n",
    "distance_network.set_pois(category='employment', maxdist=maximum_dist, maxitems=100, x_col=x, y_col=y)\n",
    "\n",
    "employment_access = distance_network.nearest_pois(\n",
    "    distance=maximum_dist, category='employment', num_pois=100, include_poi_ids=True)\n",
    "\n",
    "jobcounts = employment_access.iloc[:,100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b9b9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight, distance=maximum_dist))*\n",
    "                                jobcounts.values\n",
    "                                ).sum(axis=1)\n",
    "\n",
    "weight = 100*poi_weights['employment'][0]/sum(sum(list(poi_weights.values()),[]))\n",
    "\n",
    "results['employment'] = weight*results['jobs']/70000\n",
    "\n",
    "results['Walk_Index'] = results['Walk_Index'] + results['employment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e5b9e",
   "metadata": {},
   "source": [
    "#### below redundant now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "438bb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (employment_centrs['geometry'].x, employment_centrs['geometry'].y)\n",
    "\n",
    "distance_network.set_pois(category='employment', maxdist=maximum_dist, maxitems=100, x_col=x, y_col=y)\n",
    "\n",
    "employment_access = distance_network.nearest_pois(\n",
    "    distance=maximum_dist, category='employment', num_pois=100, include_poi_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c501b",
   "metadata": {},
   "source": [
    "The nearest_pois function returns both distances and the IDs of the nearest pois (with include_poi_ids option). The IDs can then be used to retrieve the number of jobs at each point. I found a merge was the fastest way to join this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c196467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itermerge(dataframe, jobs):\n",
    "    i=0\n",
    "    for column in dataframe:\n",
    "        dataframe = dataframe.merge(jobs, how='left', left_on = column, right_index = True, suffixes = [None, i])\n",
    "        i = i + 1\n",
    "    return dataframe\n",
    "\n",
    "def access_weight(x, distance):\n",
    "    beta = -0.001\n",
    "    if x == distance:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(beta*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02b8a612",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-35781f13368e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjobcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitermerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memployment_access\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memployment_centrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Jobs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight))*\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mjobcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 ).sum(axis=1)\n",
      "\u001b[1;32m<ipython-input-58-0c3b7b07ba02>\u001b[0m in \u001b[0;36mitermerge\u001b[1;34m(dataframe, jobs)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ox\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9189\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9191\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m   9192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9193\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ox\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m ) -> DataFrame:\n\u001b[1;32m--> 105\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ox\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# to avoid incompatible dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ox\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1253\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m                 ):\n\u001b[1;32m-> 1255\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "jobcounts = itermerge(employment_access.iloc[:,100:200], employment_centrs['Jobs'])\n",
    "\n",
    "results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight))*\n",
    "                                jobcounts.iloc[:,100:200].values\n",
    "                                ).sum(axis=1)\n",
    "\n",
    "weight = 100*sum(poi_weights['employment'])/sum(sum(list(poi_weights.values()),[]))\n",
    "\n",
    "results['employment'] = weight*results['jobs']/max(results['jobs'])\n",
    "\n",
    "results['Walk_Index'] = results['Walk_Index'] + results['employment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d110b0",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfcd47",
   "metadata": {},
   "source": [
    "Filter the results to the original Colouring Sydney buildings only. Optionally export results as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ec2e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results = results.filter(items=nodes[nodes['connect_type'] == 'poi'].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be4569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results.to_csv(\"Colouring_bf_results_010522.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb824b",
   "metadata": {},
   "source": [
    "Import building footprints and join the data to them, then export these polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f943788",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = gpd.GeoDataFrame(building_results, geometry = gpd.GeoSeries.from_xy(building_results.x, building_results.y, crs=\"EPSG:7856\"))\n",
    "\n",
    "buildings_foot = gpd.read_file(folder + \"adelaide_bf.shp\").to_crs(proj_crs)\n",
    "\n",
    "# join to data\n",
    "buildings_foot = gpd.sjoin(buildings_foot, results_gdf, how='left', predicate='contains')\n",
    "\n",
    "buildings_foot.to_file(\"Colouring_bf_results_180221.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bf109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
