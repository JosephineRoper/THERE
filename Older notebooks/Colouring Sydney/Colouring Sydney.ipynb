{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e167a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec4193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import sys\n",
    "# adding functions \n",
    "sys.path.insert(0, 'C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Open-Walk-Index')\n",
    "from walkability_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40338e69",
   "metadata": {},
   "source": [
    "Choose a projected CRS to be used for all distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28571a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_crs = \"EPSG:7856\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865078a9",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Data sources:\n",
    "1. Shape of Greater Sydney - used to clip points\n",
    "2. Points of interest from OSM\n",
    "3. Transport for NSW public transport stops\n",
    "4. Spatial Services NSW for additional POIs\n",
    "5. Employment data - processed from ABS originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\\"\n",
    "Greater_Sydney = gpd.read_file((folder + \n",
    "    \"Data\\\\greater_sydney_shape\\\\Greater_Sydney_Dissolve.shp\")\n",
    "    ).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c494196",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_poi_points = gpd.read_file(''.join((folder, \n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_POIs_sp.shp\")))\n",
    "osm_poi_areas = gpd.read_file(folder + \n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_POIs_a_sp.gpkg\")\n",
    "osm_transport_points = gpd.read_file(folder +\n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_transport_sp.shp\")\n",
    "osm_transport_areas =  gpd.read_file(folder +\n",
    "    \"Data\\\\OSM-australia-latest-free\\\\gis_osm_transport_a_free_1.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a68580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfnsw = gpd.read_file(folder +\n",
    "                    \"Data\\\\TfNSW\\\\full_greater_sydney_gtfs_static\\\\unique_stops_types.csv\")\n",
    "tfnsw.geometry = gpd.points_from_xy(tfnsw['stop_lon'], tfnsw['stop_lat'])\n",
    "tfnsw.crs = \"EPSG:7843\"\n",
    "tfnsw = tfnsw.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0e4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SS NSW data\n",
    "SS_NSW = gpd.read_file(\"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Data\\\\NSW Spatial Services\\\\NSW_Features_of_Interest_Category.gdb\",layer='BuildingComplexPoint')\n",
    "SS_NSW.to_crs(Greater_Sydney.crs, inplace = True)\n",
    "SS_NSW['fclass'] = (SS_NSW['classsubtype'].astype(str) + \"-\" \n",
    "                    + SS_NSW['buildingcomplextype'].astype(str))\n",
    "\n",
    "# add general cultural points\n",
    "SS_NSW_gc = gpd.read_file(\"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Data\\\\NSW Spatial Services\\\\NSW_Features_of_Interest_Category.gdb\",layer='GeneralCulturalPoint')\n",
    "SS_NSW_gc.to_crs(Greater_Sydney.crs, inplace = True)\n",
    "SS_NSW_gc['fclass'] = (SS_NSW_gc['classsubtype'].astype(str) + \"-\" \n",
    "                       + SS_NSW_gc['generalculturaltype'].astype(str) + \"-gc\")\n",
    "\n",
    "SS_NSW = pd.concat([SS_NSW, SS_NSW_gc]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = gpd.read_file(folder +\n",
    "    \"Data\\\\Colouring\\\\Centroids employment MBs.gpkg\").to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48e4c3",
   "metadata": {},
   "source": [
    "Convert polygonal datasets to points and any multipart datasets to single part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ebdc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_pois_2 = single_points(osm_poi_areas)\n",
    "osm_transport_2 = single_points(osm_transport_areas)\n",
    "\n",
    "osm_df = pd.concat([osm_poi_points, osm_pois_2, osm_transport_points, \n",
    "                    osm_transport_2]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31928c3",
   "metadata": {},
   "source": [
    "### Categorise and weight POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70870",
   "metadata": {},
   "source": [
    "Categorise POI data - change classes depending on your analysis and your data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e61d660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "['4-6' '2-24' '5-9' '2-0' '5-10' '2-8' '2-12' '3-0' '2-4' '6-3' '2-17'\n",
      " '2-21' '2-20' '2-9' '5-8' '4-7' '4-4' '4-8' '2-23' '1-0' '3-1' '2-19'\n",
      " '2-5' '4-0' '2-6' '2-7' '2-1' '5-1' '4-1' '2-10' '2-15' '5-5' '5-11'\n",
      " '5-2' '4-2' '4-3' '5-3' '4-9' '5-6' '3-4' '5-7' '6-0' '5-0' '6-2'\n",
      " '4-11-gc' '12-0-gc' '8-0-gc' '4-12-gc' '1-4-gc' '4-7-gc' '7-0-gc'\n",
      " '4-6-gc' '4-2-gc' '8-2-gc' '6-5-gc' '1-14-gc' '12-1-gc' '1-1-gc' '7-2-gc'\n",
      " '7-1-gc' '1-11-gc' '4-9-gc' '4-3-gc' '5-0-gc' '5-1-gc' '4-0-gc' '4-4-gc'\n",
      " '6-2-gc' '2-0-gc' '4-8-gc' '2-1-gc' '8-1-gc' '1-3-gc' '3-1-gc' '6-4-gc'\n",
      " '7-3-gc' '4-5-gc' '2-2-gc' '6-7-gc' '7-4-gc' '6-8-gc' '6-6-gc' '6-3-gc'\n",
      " '2-3-gc' '1-10-gc' '4-13-gc' '1-0-gc' '7-5-gc' '9-0-gc' '9-1-gc' '6-1-gc'\n",
      " '3-0-gc' '6-0-gc']\n"
     ]
    }
   ],
   "source": [
    "SS_names = {'shopping centre':[\"4-10\"], 'post office':[\"2-18\"],'place of worship':[\"2-16\"], 'health centre':[\"3-3\", \"3-5\", \"3-6\"], \n",
    "                 'school':[\"1-1\", \"1-2\", \"1-3\", \"1-5\", \"1-6\", \"1-8\"], 'university':[\"1-4\"], 'childcare':[\"1-7\"],\n",
    "                 'art gallery':[\"2-2\"], 'library': [\"2-11\"], 'museum':[\"2-14\"], 'sports centre':[\"6-12\"], \n",
    "                 'zoo':[\"6-18\"], 'outdoor theater':[\"1-6-gc\"], 'swimming pool':[\"6-15\", \"9-2-gc\"], \n",
    "                 'tourist attraction':[\"6-17\"], 'golf course':[\"1-2-gc\"], 'lookout':[\"1-5-gc\"], \n",
    "                 'park':[\"1-7-gc\"], 'picnic area':[\"1-8-gc\"], 'sports field':[\"1-12-gc\", \"1-13-gc\"]}\n",
    "SS_NSW = categorise_pois(SS_NSW, SS_names, old_column='fclass', new_column='fclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d17999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "['nan']\n",
      "Tags present in the dataset but not categorised:\n",
      "['place of worship' 'tourist attraction' 'swimming pool' 'school'\n",
      " 'shopping centre' 'post office' 'sports centre' 'library' 'university'\n",
      " 'museum' 'health centre' 'zoo' 'art gallery' 'childcare' 'lookout'\n",
      " 'picnic area' 'park' 'sports field' 'golf course' 'outdoor theater']\n",
      "Tags present in the dataset but not categorised:\n",
      "['drinking_water' 'toilet' 'memorial' 'public_building' 'monument'\n",
      " 'police' 'motel' 'shelter' 'tourist_info' 'bicycle_rental' 'hostel'\n",
      " 'hotel' 'telephone' 'recycling' 'guesthouse' 'lighthouse' 'courthouse'\n",
      " 'fire_station' 'camp_site' 'town_hall' 'fountain' 'caravan_site' 'bench'\n",
      " 'water_tower' 'comms_tower' 'tower' 'waste_basket' 'graveyard'\n",
      " 'wastewater_plant' 'camera_surveillance' 'nursing_home' 'car_rental'\n",
      " 'recycling_clothes' 'vending_parking' 'battlefield' 'chalet'\n",
      " 'vending_any' 'prison' 'vending_machine' 'water_works' 'embassy'\n",
      " 'water_well' 'windmill' 'recycling_glass' 'hunting_stand' 'water_mill'\n",
      " 'fort' 'recycling_paper' 'taxi']\n"
     ]
    }
   ],
   "source": [
    "tfnsw_categories = {'transport':['bus'], 'trains':['train','ferry']}\n",
    "\n",
    "tfnsw_categorised = categorise_pois(tfnsw, tfnsw_categories, \n",
    "                                 old_column='service_type', new_column='category')\n",
    "\n",
    "SS_categories = {'shopping':[\"4-10\"], \n",
    "                 'errands':[\"2-18\", \"2-16\", \"3-3\", \"3-5\", \"3-6\"], \n",
    "                 'education':[\"1-1\", \"1-2\", \"1-3\", \"1-4\", \"1-8\"], \n",
    "                 'entertainment':[\"2-2\", \"2-11\", \"2-14\", \"6-12\", \"6-18\", \"1-6-gc\"], \n",
    "                 'parks':[\"6-15\", \"6-17\", \"1-2-gc\", \"1-5-gc\", \"1-7-gc\", \n",
    "                           \"1-8-gc\", \"1-12-gc\", \"1-13-gc\", \"9-2-gc\"]}\n",
    "\n",
    "SS_categorised = categorise_pois(SS_NSW, SS_categories, \n",
    "                                 old_column='fclass', new_column='category')\n",
    "\n",
    "osm_categories = {\"eating\" : ['restaurant', 'pub', 'cafe', 'fast_food', \n",
    "                              'food_court', 'bakery', 'bar', 'nightclub', 'biergarten'], \n",
    "                  'groceries' : ['supermarket', 'chemist', 'pharmacy', 'greengrocer', \n",
    "                                 'convenience', 'butcher', 'beverages', 'alcohol'], \n",
    "                  'shopping' : ['mall', 'bicycle_shop', 'clothes', \n",
    "                                'department_store', 'doityourself', \n",
    "                                'outdoor_shop', 'stationery', 'bookshop', \n",
    "                                'gift_shop', 'newsagent', 'car_dealership', \n",
    "                                'kiosk', 'furniture_shop', 'sports_shop', \n",
    "                                'garden_centre', 'computer_shop', 'shoe_shop', \n",
    "                                'beauty_shop', 'florist', 'video_shop', 'toy_shop', \n",
    "                                'mobile_phone_shop', 'jeweller', 'travel_agent'], \n",
    "                  'errands' : ['post_box', 'post_office', 'bank', 'atm',\n",
    "                               'doctors', 'dentist', 'laundry', 'hospital',\n",
    "                               'car_wash', 'veterinary', 'hairdresser', 'optician'], \n",
    "                  'parks' : ['viewpoint', 'park', 'playground', 'picnic_site', \n",
    "                             'pitch', 'swimming_pool', 'sports_centre', \n",
    "                             'golf_course', 'track', 'dog_park'], \n",
    "                  'education' : ['college', 'school', 'kindergarten', 'university'], \n",
    "                  'entertainment' : ['library', 'attraction', 'stadium', \n",
    "                                     'arts_centre', 'theatre', 'artwork', \n",
    "                                     'archaeological', 'cinema', 'museum', \n",
    "                                     'ruins', 'observation_tower', \n",
    "                                     'community_centre', 'zoo', 'castle', \n",
    "                                     'theme_park', 'ice_rink'], \n",
    "                 'trains' : ['ferry_terminal', 'railway_station', 'bus_station', \n",
    "                             'tram_stop', 'railway_halt', 'publictransport'], \n",
    "                 'transport' : ['car_sharing', 'bus_stop']}\n",
    "\n",
    "osm_categorised = categorise_pois(osm_df, osm_categories, \n",
    "                                  old_column='fclass', new_column='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1bf6",
   "metadata": {},
   "source": [
    "Need to remove potential overlap between different data sources (and inside some data sources). For this dataset it's around 30% because there is overlap of public transport stops between OSM and TfNSW, and overlap of things like post offices between OSM and SSNSW. Then take this combined POI set and clip it to the study area: should be the same area as is covered by the network. This is important otherwise points outside the network may be erroneously linked to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747397b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8.68% duplicate points from dataframes\n"
     ]
    }
   ],
   "source": [
    "pois = remove_duplicate_pois([osm_categorised, SS_categorised, \n",
    "                              tfnsw_categorised], buffer=10)\n",
    "\n",
    "pois = gpd.clip(pois, Greater_Sydney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ca191",
   "metadata": {},
   "source": [
    "Choose walk index weightings, and output the sums of each category and the total to check. The walk index will be out of 100 regardless of this sum, but it is important to note that eg. shopping is only '10% of the walk index' if shopping is 10 out of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd0dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_weights = {\n",
    "    \"employment\": [10],\n",
    "    \"eating\": [3, 3, 3, 2, 2, 1, 1, 1, 1, 1],\n",
    "    \"groceries\": [10, 4],\n",
    "    \"shopping\": [2, 2, 2, 2, 2],\n",
    "    \"errands\": [6, 2, 4],\n",
    "    \"parks\": [6],\n",
    "    \"education\": [10],\n",
    "    \"entertainment\": [5],\n",
    "    \"trains\": [10],\n",
    "    \"transport\": [2.5, 2.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2440fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'employment': 10, 'eating': 18, 'groceries': 14, 'shopping': 10, 'errands': 12, 'parks': 6, 'education': 10, 'entertainment': 5, 'trains': 10, 'transport': 5.0}\n",
      "total:  100.0\n"
     ]
    }
   ],
   "source": [
    "category_sums = {k: sum(v) for k, v in poi_weights.items()}\n",
    "total = sum(category_sums.values())\n",
    "print(category_sums)\n",
    "print(\"total: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd7581",
   "metadata": {},
   "source": [
    "### Import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b6f48",
   "metadata": {},
   "source": [
    "In this case the network is already in the same projected CRS as everything else but I have left in the transformation to be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d529acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (2,4,5,7,9,13,14,19,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (6,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# reading directly with geopandas.read_file crashes on my computer so I read into pandas then convert to gdf instead\n",
    "data = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Colouring data & results\\\\Sydney Data\\\\\"\n",
    "edges_df = pd.read_csv(data + \"Data\\\\colouring_edges_150322.csv\")\n",
    "nodes_df = pd.read_csv(data + \"Data\\\\colouring_nodes_150322.csv\")\n",
    "edges = gpd.GeoDataFrame(edges_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(edges_df['geometry'])).set_crs(proj_crs)\n",
    "nodes = gpd.GeoDataFrame(nodes_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(nodes_df['geometry'])).set_crs(proj_crs)\n",
    "edges = edges.to_crs(proj_crs)\n",
    "nodes = nodes.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395d9f",
   "metadata": {},
   "source": [
    "Pandana expects edges to have a two item index based on the same IDs as the node index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31dda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.set_index('connect_id',inplace=True)\n",
    "\n",
    "edges['from_idx'] = edges['from']\n",
    "edges['to_idx'] = edges['to']\n",
    "edges= edges.set_index(['from_idx', 'to_idx'])\n",
    "edges.index.names= ['from_idx','to_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea582800",
   "metadata": {},
   "source": [
    "Pandana network creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1149fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_network = pdna.Network(nodes['x'], nodes['y'],\n",
    "                                   edges['from'], edges['to'], \n",
    "                                   edges[['length']])\n",
    "\n",
    "maximum_dist = 2400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4efc6",
   "metadata": {},
   "source": [
    "Pandana network querying. The 'employment' category is empty because we didn't add the employment points to the POI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63e13f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category employment is empty\n",
      "Finished category: eating\n",
      "Finished category: groceries\n",
      "Finished category: shopping\n",
      "Finished category: errands\n",
      "Finished category: parks\n",
      "Finished category: education\n",
      "Finished category: entertainment\n",
      "Finished category: trains\n",
      "Finished category: transport\n"
     ]
    }
   ],
   "source": [
    "results = walk_index_m(distance_network, pois, poi_weights, distance=maximum_dist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc063b9",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8af95",
   "metadata": {},
   "source": [
    "The current approach is to find up to 100 closest employment nodes within the maximum distance. Then look up the number of jobs at each one, apply a distance decay function to each distance, multiply these together, and sum.\n",
    "\n",
    "An alternative approach which would be more convenient would be to use the Pandana 'aggregate' function which aggregates from all nodes within the maximum distance. However, there is limited ability to change the distance decay rate within the aggregation function. It can either be flat (no decay), linear (going to 0 at the max distance), or exponential where beta is set as 1/max distance. For walking I would like a beta of 0.001, but this requires the radius to be 1000m. If the radius is 2400m, beta is only 0.0004. This can be changed in the future if the Pandana function is updated to take a decay parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "438bb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (employment_centrs['geometry'].x, employment_centrs['geometry'].y)\n",
    "\n",
    "distance_network.set_pois(category='employment', maxdist=maximum_dist, maxitems=100, x_col=x, y_col=y)\n",
    "\n",
    "employment_access = distance_network.nearest_pois(\n",
    "    distance=maximum_dist, category='employment', num_pois=100, include_poi_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c501b",
   "metadata": {},
   "source": [
    "The nearest_pois function returns both distances and the IDs of the nearest pois (with include_poi_ids option). The IDs can then be used to retrieve the number of jobs at each point. I found a merge was the fastest way to join this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c196467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itermerge(dataframe, jobs):\n",
    "    i=0\n",
    "    for column in dataframe:\n",
    "        dataframe = dataframe.merge(jobs, how='left', left_on = column, right_index = True, suffixes = [None, i])\n",
    "        i = i + 1\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02b8a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobcounts = itermerge(employment_access.iloc[:,100:200], employment_centrs['Jobs_count'])\n",
    "\n",
    "results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight, distance=maximum_dist))*\n",
    "                                jobcounts.iloc[:,100:200].values\n",
    "                                ).sum(axis=1)\n",
    "\n",
    "weight = 100*sum(poi_weights['employment'])/sum(sum(list(poi_weights.values()),[]))\n",
    "\n",
    "results['employment'] = weight*results['jobs']/max(results['jobs'])\n",
    "\n",
    "results['Walk_Index'] = results['Walk_Index'] + results['employment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d110b0",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfcd47",
   "metadata": {},
   "source": [
    "Filter the results to the original Colouring Sydney buildings only. Optionally export results as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ec2e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results = results.filter(items=nodes[nodes['connect_type'] == 'poi'].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be4569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results.to_csv(\"Colouring_results_250321.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb824b",
   "metadata": {},
   "source": [
    "Import building footprints and join the data to them, then export these polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f943788",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = gpd.GeoDataFrame(building_results, geometry = gpd.GeoSeries.from_xy(building_results.x, building_results.y, crs=\"EPSG:7856\"))\n",
    "\n",
    "buildings_foot = gpd.read_file(folder +\n",
    "    \"Data\\\\Colouring\\\\Building Footprints\\\\sydney_bf.shp\").to_crs(proj_crs)\n",
    "\n",
    "# join to data\n",
    "buildings_foot = gpd.sjoin(buildings_foot, results_gdf, how='left', predicate='contains')\n",
    "\n",
    "buildings_foot.to_file(\"Colouring_bf_results_250321.gpkg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
