{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68147bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec4193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import sys\n",
    "# adding functions \n",
    "sys.path.insert(0, 'C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Open-Walk-Index')\n",
    "from walkability_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40338e69",
   "metadata": {},
   "source": [
    "Choose a projected CRS to be used for all distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28571a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_crs = \"EPSG:7856\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865078a9",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Data sources:\n",
    "1. Shape of Greater Sydney - used to clip points\n",
    "2. Points of interest from OSM\n",
    "3. Transport for NSW public transport stops\n",
    "4. Spatial Services NSW for additional POIs\n",
    "5. Employment data - processed from ABS originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\\"\n",
    "Greater_Sydney = gpd.read_file((folder + \n",
    "    \"Data\\\\greater_sydney_shape\\\\Greater_Sydney_Dissolve.shp\")\n",
    "    ).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c494196",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_poi_points = gpd.read_file(''.join((folder, \n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_POIs_sp.shp\")))\n",
    "osm_poi_areas = gpd.read_file(folder + \n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_POIs_a_sp.gpkg\")\n",
    "osm_transport_points = gpd.read_file(folder +\n",
    "    \"Walkability\\\\Mavoa\\\\Pandana inputs\\\\Greater_Sydney_OSM_transport_sp.shp\")\n",
    "osm_transport_areas =  gpd.read_file(folder +\n",
    "    \"Data\\\\OSM-australia-latest-free\\\\gis_osm_transport_a_free_1.shp\")\n",
    "\n",
    "# the OSM POIs includes domestic swimming pools in some suburbs. This line removes swimming pools less than 100m2.\n",
    "# Same for domestic tennis courts appearing as 'pitches'. Removed pitches below 450m2.\n",
    "osm_poi_areas = osm_poi_areas[~((osm_poi_areas['fclass']=='swimming_pool') & (osm_poi_areas.to_crs(proj_crs).area < 100))]\n",
    "osm_poi_areas = osm_poi_areas[~((osm_poi_areas['fclass']=='pitch') & (osm_poi_areas.to_crs(proj_crs).area < 450))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239bea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SS NSW data\n",
    "SS_NSW = gpd.read_file((folder + \n",
    "                        \"Data\\\\NSW Spatial Services\\\\NSW_Features_of_Interest_Category.gdb\"),layer='BuildingComplexPoint')\n",
    "SS_NSW.to_crs(Greater_Sydney.crs, inplace = True)\n",
    "SS_NSW['fclass'] = (SS_NSW['classsubtype'].astype(str) + \"-\" \n",
    "                    + SS_NSW['buildingcomplextype'].astype(str))\n",
    "\n",
    "# add general cultural points\n",
    "SS_NSW_gc = gpd.read_file((folder + \n",
    "                           \"Data\\\\NSW Spatial Services\\\\NSW_Features_of_Interest_Category.gdb\"),layer='GeneralCulturalPoint')\n",
    "SS_NSW_gc.to_crs(Greater_Sydney.crs, inplace = True)\n",
    "SS_NSW_gc['fclass'] = (SS_NSW_gc['classsubtype'].astype(str) + \"-\" \n",
    "                       + SS_NSW_gc['generalculturaltype'].astype(str) + \"-gc\")\n",
    "\n",
    "SS_NSW = pd.concat([SS_NSW, SS_NSW_gc]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0e4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_names = {'shopping centre':[\"4-10\"], 'post office':[\"2-18\"],'place of worship':[\"2-16\"], 'health centre':[\"3-3\", \"3-5\", \"3-6\"], \n",
    "                 'school':[\"1-1\", \"1-2\", \"1-3\", \"1-5\", \"1-6\", \"1-8\"], 'university':[\"1-4\"], 'childcare':[\"1-7\"],\n",
    "                 'art gallery':[\"2-2\"], 'library': [\"2-11\"], 'museum':[\"2-14\"], 'sports centre':[\"6-12\"], \n",
    "                 'zoo':[\"6-18\"], 'outdoor theater':[\"1-6-gc\"], 'swimming pool':[\"6-15\", \"9-2-gc\"], \n",
    "                 'tourist attraction':[\"6-17\"], 'golf course':[\"1-2-gc\"], 'lookout':[\"1-5-gc\"], \n",
    "                 'park':[\"1-7-gc\"], 'picnic area':[\"1-8-gc\"], 'sports field':[\"1-12-gc\", \"1-13-gc\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea8fe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "['4-6' '2-24' '5-9' '2-0' '5-10' '2-8' '2-12' '3-0' '2-4' '6-3' '2-17'\n",
      " '2-21' '2-20' '2-9' '5-8' '4-7' '4-4' '4-8' '2-23' '1-0' '3-1' '2-19'\n",
      " '2-5' '4-0' '2-6' '2-7' '2-1' '5-1' '4-1' '2-10' '2-15' '5-5' '5-11'\n",
      " '5-2' '4-2' '4-3' '5-3' '4-9' '5-6' '3-4' '5-7' '6-0' '5-0' '6-2'\n",
      " '4-11-gc' '12-0-gc' '8-0-gc' '4-12-gc' '1-4-gc' '4-7-gc' '7-0-gc'\n",
      " '4-6-gc' '4-2-gc' '8-2-gc' '6-5-gc' '1-14-gc' '12-1-gc' '1-1-gc' '7-2-gc'\n",
      " '7-1-gc' '1-11-gc' '4-9-gc' '4-3-gc' '5-0-gc' '5-1-gc' '4-0-gc' '4-4-gc'\n",
      " '6-2-gc' '2-0-gc' '4-8-gc' '2-1-gc' '8-1-gc' '1-3-gc' '3-1-gc' '6-4-gc'\n",
      " '7-3-gc' '4-5-gc' '2-2-gc' '6-7-gc' '7-4-gc' '6-8-gc' '6-6-gc' '6-3-gc'\n",
      " '2-3-gc' '1-10-gc' '4-13-gc' '1-0-gc' '7-5-gc' '9-0-gc' '9-1-gc' '6-1-gc'\n",
      " '3-0-gc' '6-0-gc']\n"
     ]
    }
   ],
   "source": [
    "SS_NSW = categorise_pois(SS_NSW, SS_names, 'fclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d8bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = gpd.read_file(folder +\n",
    "    \"Data\\\\Colouring\\\\Centroids employment MBs.gpkg\").to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48e4c3",
   "metadata": {},
   "source": [
    "Convert polygonal datasets to points and any multipart datasets to single part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ebdc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_pois_2 = single_points(osm_poi_areas)\n",
    "osm_transport_2 = single_points(osm_transport_areas)\n",
    "\n",
    "osm_df = pd.concat([osm_poi_points, osm_pois_2, osm_transport_points, \n",
    "                    osm_transport_2]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31928c3",
   "metadata": {},
   "source": [
    "### Categorise and weight POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70870",
   "metadata": {},
   "source": [
    "Categorise POI data - change classes depending on your analysis and your data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19d17999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "[]\n",
      "Tags present in the dataset but not categorised:\n",
      "['drinking_water' 'toilet' 'memorial' 'public_building' 'monument'\n",
      " 'police' 'motel' 'shelter' 'tourist_info' 'bicycle_rental' 'hostel'\n",
      " 'hotel' 'telephone' 'recycling' 'guesthouse' 'lighthouse' 'courthouse'\n",
      " 'fire_station' 'camp_site' 'town_hall' 'fountain' 'caravan_site' 'bench'\n",
      " 'water_tower' 'comms_tower' 'tower' 'waste_basket' 'graveyard'\n",
      " 'wastewater_plant' 'camera_surveillance' 'nursing_home' 'car_rental'\n",
      " 'recycling_clothes' 'vending_parking' 'battlefield' 'chalet'\n",
      " 'vending_any' 'prison' 'vending_machine' 'water_works' 'embassy'\n",
      " 'water_well' 'windmill' 'recycling_glass' 'car_sharing' 'hunting_stand'\n",
      " 'water_mill' 'fort' 'recycling_paper' 'railway_station' 'bus_station'\n",
      " 'bus_stop' 'ferry_terminal' 'railway_halt' 'taxi' 'tram_stop']\n"
     ]
    }
   ],
   "source": [
    "SS_categories = {'shopping':[\"shopping centre\"], \n",
    "                 'errands':[\"post office\", \"place of worship\", \"health centre\"], \n",
    "                 'education':[\"school\", \"university\", \"childcare\"], \n",
    "                 'recreation':[\"art gallery\", \"library\", \"museum\", \"sports centre\", \"sports field\",\n",
    "                                 \"zoo\", \"outdoor theater\", \"swimming pool\", \"tourist attraction\",\n",
    "                                 \"golf course\", \"lookout\", \"park\", \"picnic area\"]}\n",
    "\n",
    "SS_categorised = categorise_pois(SS_NSW, SS_categories, \n",
    "                                 category_column='fclass')\n",
    "\n",
    "osm_categories = {'shopping' : ['supermarket', 'chemist', 'pharmacy', 'greengrocer', \n",
    "                                 'convenience', 'butcher', 'beverages', 'alcohol',\n",
    "                                'mall', 'bicycle_shop', 'clothes', \n",
    "                                'department_store', 'doityourself', \n",
    "                                'outdoor_shop', 'stationery', 'bookshop', \n",
    "                                'gift_shop', 'newsagent', 'car_dealership', \n",
    "                                'kiosk', 'furniture_shop', 'sports_shop', \n",
    "                                'garden_centre', 'computer_shop', 'shoe_shop', \n",
    "                                'beauty_shop', 'florist', 'video_shop', 'toy_shop', \n",
    "                                'mobile_phone_shop', 'jeweller', 'travel_agent'], \n",
    "                  'errands' : ['post_box', 'post_office', 'bank', 'atm',\n",
    "                               'doctors', 'dentist', 'laundry', 'hospital',\n",
    "                               'car_wash', 'veterinary', 'hairdresser', 'optician'], \n",
    "                  'education' : ['college', 'school', 'kindergarten', 'university'], \n",
    "                  'recreation' : ['restaurant', 'pub', 'cafe', 'fast_food', \n",
    "                              'food_court', 'bakery', 'bar', 'nightclub', 'biergarten',\n",
    "                                  'library', 'attraction', 'stadium', \n",
    "                                     'arts_centre', 'theatre', 'artwork', \n",
    "                                     'archaeological', 'cinema', 'museum', \n",
    "                                     'ruins', 'observation_tower', \n",
    "                                     'community_centre', 'zoo', 'castle', \n",
    "                                     'theme_park', 'ice_rink','viewpoint', 'park', 'playground', 'picnic_site', \n",
    "                             'pitch', 'swimming_pool', 'sports_centre', \n",
    "                             'golf_course', 'track', 'dog_park']}\n",
    "\n",
    "osm_categorised = categorise_pois(osm_df, osm_categories, \n",
    "                                  category_column='fclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1bf6",
   "metadata": {},
   "source": [
    "Need to remove potential overlap between different data sources (and inside some data sources). For this dataset it's around 30% because there is overlap of public transport stops between OSM and TfNSW, and overlap of things like post offices between OSM and SSNSW. Then take this combined POI set and clip it to the study area: should be the same area as is covered by the network. This is important otherwise points outside the network may be erroneously linked to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "747397b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 9.80% duplicate points from dataframes\n"
     ]
    }
   ],
   "source": [
    "pois = remove_duplicate_pois([osm_categorised, SS_categorised], buffer=10)\n",
    "\n",
    "pois = gpd.clip(pois, Greater_Sydney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ca191",
   "metadata": {},
   "source": [
    "Choose walk index weightings, and output the sums of each category and the total to check. The walk index will be out of 100 regardless of this sum, but it is important to note that eg. shopping is only '10% of the walk index' if shopping is 10 out of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5de511d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fclass\n",
       "education      3619\n",
       "errands        4556\n",
       "recreation    28905\n",
       "shopping       4171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_opps = pois.groupby(['fclass']).size()\n",
    "no_opps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3967fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois[pois['fclass']=='shopping'].to_file(\"sydney_shopping.gpkg\")\n",
    "pois[pois['fclass']=='recreation'].to_file(\"sydney_recreation.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa5a7f",
   "metadata": {},
   "source": [
    "Therefore for Sydney we posit that reaching 5 shops, 28 recreational opportunities, 5 personal business opportunities and 4 schools or universities is roughly equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd0dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_weights = {\n",
    "    \"employment\": [29],\n",
    "    \"education\": [13],\n",
    "    \"shopping\": [19],\n",
    "    \"errands\": [7],\n",
    "    \"recreation\": [32],\n",
    "}\n",
    "\n",
    "poi_numbers = {\n",
    "    \"employment\": [100],\n",
    "    \"education\": [4],\n",
    "    \"shopping\": [5],\n",
    "    \"errands\": [5],\n",
    "    \"recreation\": [28],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2440fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'employment': 29, 'education': 13, 'shopping': 19, 'errands': 7, 'recreation': 32}\n",
      "total:  100\n"
     ]
    }
   ],
   "source": [
    "category_sums = {k: sum(v) for k, v in poi_weights.items()}\n",
    "total = sum(category_sums.values())\n",
    "print(category_sums)\n",
    "print(\"total: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd7581",
   "metadata": {},
   "source": [
    "### Import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b6f48",
   "metadata": {},
   "source": [
    "In this case the network is already in the same projected CRS as everything else but I have left in the transformation to be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d529acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (2,4,5,7,9,13,14,19,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (6,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# reading directly with geopandas.read_file crashes on my computer so I read into pandas then convert to gdf instead\n",
    "edges_df = pd.read_csv(folder + \"Walkability//Other Cities//Colouring data & results//Sydney data//Data//colouring_edges_150322.csv\")\n",
    "nodes_df = pd.read_csv(folder + \"Walkability//Other Cities//Colouring data & results//Sydney data//Data//colouring_nodes_150322.csv\")\n",
    "edges = gpd.GeoDataFrame(edges_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(edges_df['geometry'])).set_crs(proj_crs)\n",
    "nodes = gpd.GeoDataFrame(nodes_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(nodes_df['geometry'])).set_crs(proj_crs)\n",
    "edges = edges.to_crs(proj_crs)\n",
    "nodes = nodes.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395d9f",
   "metadata": {},
   "source": [
    "Pandana expects edges to have a two item index based on the same IDs as the node index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31dda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.set_index('connect_id',inplace=True)\n",
    "\n",
    "edges['from_idx'] = edges['from']\n",
    "edges['to_idx'] = edges['to']\n",
    "edges= edges.set_index(['from_idx', 'to_idx'])\n",
    "edges.index.names= ['from_idx','to_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea582800",
   "metadata": {},
   "source": [
    "## Pandana network creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1149fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_network = pdna.Network(nodes['x'], nodes['y'],\n",
    "                                   edges['from'], edges['to'], \n",
    "                                   edges[['length']])\n",
    "\n",
    "maximum_dist = 2400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4efc6",
   "metadata": {},
   "source": [
    "Pandana network querying. The 'employment' category is empty because we didn't add the employment points to the POI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e13f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = walk_index(distance_network, pois, poi_weights, distance=maximum_dist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e64e5",
   "metadata": {},
   "source": [
    "Get distributions of number of reachable destinations in each category across the network, to check whether the max destinations we use is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d85f9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'education'\n",
    "# the name bit is purely to reduce the number of points for now. \n",
    "x, y = (pois[(pois.fclass == category) &(~pois['name'].isna())]['geometry'].x, \n",
    "                pois[(pois.fclass == category) &(~pois['name'].isna())]['geometry'].y)\n",
    "node_ids = distance_network.get_node_ids(x, y, mapping_distance=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "988af2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24019    1632569\n",
       "30828    1893120\n",
       "10394    3947272\n",
       "30842    1474188\n",
       "22782    2295155\n",
       "          ...   \n",
       "26121    2200541\n",
       "10366    3947244\n",
       "30869    2438956\n",
       "23041    1874455\n",
       "29525    2031716\n",
       "Name: node_id, Length: 1652, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5600a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_network.set(node_ids, variable=None, name='dest_cat')\n",
    "#dest_cat = distance_network.set_pois(category=category, maxdist=maximum_dist, maxitems=None, x_col=x, y_col=y)\n",
    "number_dest = distance_network.aggregate(distance=maximum_dist, type='count', decay='flat', imp_name=None, name='dest_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# education count started 4:26pm. \n",
    "# 730pm still going, with 16,567 points\n",
    "# restarted with around 1600 points, started at 10:46 pm. 11:13 still going (I don't know what I was expecting)\n",
    "number_dest.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc063b9",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8af95",
   "metadata": {},
   "source": [
    "The current approach is to find up to 100 closest employment nodes within the maximum distance. Then look up the number of jobs at each one, apply a distance decay function to each distance, multiply these together, and sum.\n",
    "\n",
    "An alternative approach which would be more convenient would be to use the Pandana 'aggregate' function which aggregates from all nodes within the maximum distance. However, there is limited ability to change the distance decay rate within the aggregation function. It can either be flat (no decay), linear (going to 0 at the max distance), or exponential where beta is set as 1/max distance. For walking I would like a beta of 0.001, but this requires the radius to be 1000m. If the radius is 2400m, beta is only 0.0004. This can be changed in the future if the Pandana function is updated to take a decay parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (employment_centrs['geometry'].x, employment_centrs['geometry'].y)\n",
    "\n",
    "distance_network.set_pois(category='employment', maxdist=maximum_dist, maxitems=100, x_col=x, y_col=y)\n",
    "\n",
    "employment_access = distance_network.nearest_pois(\n",
    "    distance=maximum_dist, category='employment', num_pois=100, include_poi_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c501b",
   "metadata": {},
   "source": [
    "The nearest_pois function returns both distances and the IDs of the nearest pois (with include_poi_ids option). The IDs can then be used to retrieve the number of jobs at each point. I found a merge was the fastest way to join this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c196467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itermerge(dataframe, jobs):\n",
    "    i=0\n",
    "    for column in dataframe:\n",
    "        dataframe = dataframe.merge(jobs, how='left', left_on = column, right_index = True, suffixes = [None, i])\n",
    "        i = i + 1\n",
    "    return dataframe\n",
    "\n",
    "def access_weight(x):\n",
    "        beta = -0.001\n",
    "        if x == maximum_dist:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.exp(beta*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobcounts = itermerge(employment_access.iloc[:,100:200], employment_centrs['Jobs_count'])\n",
    "\n",
    "results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight))*\n",
    "                                jobcounts.iloc[:,100:200].values\n",
    "                                ).sum(axis=1)\n",
    "\n",
    "weight = 100*sum(poi_weights['employment'])/sum(sum(list(poi_weights.values()),[]))\n",
    "\n",
    "results['employment'] = weight*results['jobs']/max(results['jobs'])\n",
    "\n",
    "results['Walk_Index'] = results['Walk_Index'] + results['employment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d110b0",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfcd47",
   "metadata": {},
   "source": [
    "Filter the results to the original Colouring Sydney buildings only. Optionally export results as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results = results.filter(items=nodes[nodes['connect_type'] == 'poi'].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results.to_csv(\"HTS_5_bf_results_170322_fixnet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb824b",
   "metadata": {},
   "source": [
    "Import building footprints and join the data to them, then export these polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f943788",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = gpd.GeoDataFrame(building_results, geometry = gpd.GeoSeries.from_xy(building_results.x, building_results.y, crs=\"EPSG:7856\"))\n",
    "\n",
    "buildings_foot = gpd.read_file(folder +\n",
    "    \"Data\\\\Colouring\\\\Building Footprints\\\\sydney_bf.shp\").to_crs(proj_crs)\n",
    "\n",
    "# join to data\n",
    "buildings_foot = gpd.sjoin(buildings_foot, results_gdf, how='left', predicate='contains')\n",
    "\n",
    "buildings_foot.to_file(\"HTS_5_bf_results_170322_fixnet.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eff3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
