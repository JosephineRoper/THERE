{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec4193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import sys\n",
    "# adding functions \n",
    "sys.path.insert(0, 'C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Open-Walk-Index')\n",
    "from walkability_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40338e69",
   "metadata": {},
   "source": [
    "Choose a projected CRS to be used for all distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28571a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_crs = \"EPSG:7856\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865078a9",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Data sources:\n",
    "1. Shape of Greater Adelaide - used to clip points if not already clipped to the city\n",
    "2. Points of interest from OSM\n",
    "3. Adelaide Metro public transport stops\n",
    "4. ? additional POIs - haven't found any additional sources for South Australia yet.\n",
    "5. Employment data - processed from ABS originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\\\\Adelaide Data\\\\\"\n",
    "Greater_Adelaide = gpd.read_file((folder + \n",
    "    \"Adelaide GCCSA.gpkg\")\n",
    "    ).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c494196",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_poi_points = gpd.read_file(''.join((folder, \n",
    "    \"Adelaide OSM POI points.gpkg\")))\n",
    "osm_poi_areas = gpd.read_file(folder + \n",
    "    \"Adelaide OSM POI area centroids.gpkg\")\n",
    "osm_transport_points = gpd.read_file(folder +\n",
    "    \"Adelaide OSM transport points.gpkg\")\n",
    "osm_transport_areas =  gpd.read_file(folder +\n",
    "    \"Adelaide OSM transport areas.gpkg\")\n",
    "osm_parks_vertices = gpd.read_file(''.join((\"C:\\\\Users\\\\z3258367\\\\OneDrive - UNSW\\\\#PhD\\\\Walkability\\\\Other Cities\", \n",
    "    \"\\\\Shared Aus Data\\\\OSM parks vertices.gpkg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a68580",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro = gpd.read_file(folder + \"unique_stops_types.csv\")\n",
    "metro.geometry = gpd.points_from_xy(metro['stop_lon'], metro['stop_lat'])\n",
    "metro.crs = \"EPSG:7843\"\n",
    "metro = metro.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_centrs = gpd.read_file(folder + \"SA Employment Points.gpkg\").to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48e4c3",
   "metadata": {},
   "source": [
    "Convert polygonal datasets to points and any multipart datasets to single part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebdc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_pois_2 = single_points(osm_poi_areas)\n",
    "osm_transport_2 = single_points(osm_transport_areas)\n",
    "osm_parks_vertices = single_points(osm_parks_vertices)\n",
    "\n",
    "osm_df = pd.concat([osm_poi_points, osm_pois_2, osm_transport_points, \n",
    "                    osm_transport_2, osm_parks_vertices]).to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31928c3",
   "metadata": {},
   "source": [
    "### Categorise and weight POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70870",
   "metadata": {},
   "source": [
    "Categorise POI data - change classes depending on your analysis and your data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d17999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags present in the dataset but not categorised:\n",
      "[]\n",
      "Tags present in the dataset but not categorised:\n",
      "['monument' 'camera_surveillance' 'hotel' 'town_hall' 'toilet' 'police'\n",
      " 'motel' 'courthouse' 'drinking_water' 'telephone' 'memorial' 'guesthouse'\n",
      " 'tourist_info' 'comms_tower' 'camp_site' 'car_rental' 'water_tower'\n",
      " 'recycling_glass' 'bench' 'caravan_site' 'bicycle_rental' 'fire_station'\n",
      " 'shelter' 'hostel' 'fountain' 'recycling' 'waste_basket' 'tower' 'prison'\n",
      " 'vending_machine' 'chalet' 'vending_any' 'recycling_paper' 'graveyard'\n",
      " 'embassy' 'vending_parking' 'windmill' 'public_building' 'water_works'\n",
      " 'nursing_home' 'fort' 'wastewater_plant' 'lighthouse' 'water_well' 'taxi']\n"
     ]
    }
   ],
   "source": [
    "metro_categories = {'transport':['bus'], 'trains':['train','tram']}\n",
    "\n",
    "metro_categorised = categorise_pois(metro, metro_categories, \n",
    "                                 category_column='service_type')\n",
    "\n",
    "osm_categories = {\"eating\" : ['restaurant', 'pub', 'cafe', 'fast_food', \n",
    "                              'food_court', 'bakery', 'bar', 'nightclub', 'biergarten'], \n",
    "                  'groceries' : ['supermarket', 'chemist', 'pharmacy', 'greengrocer', \n",
    "                                 'convenience', 'butcher', 'beverages', 'alcohol'], \n",
    "                  'shopping' : ['mall', 'bicycle_shop', 'clothes', \n",
    "                                'department_store', 'doityourself', \n",
    "                                'outdoor_shop', 'stationery', 'bookshop', \n",
    "                                'gift_shop', 'newsagent', 'car_dealership', \n",
    "                                'kiosk', 'furniture_shop', 'sports_shop', \n",
    "                                'garden_centre', 'computer_shop', 'shoe_shop', \n",
    "                                'beauty_shop', 'florist', 'video_shop', 'toy_shop', \n",
    "                                'mobile_phone_shop', 'jeweller', 'travel_agent'], \n",
    "                  'errands' : ['post_box', 'post_office', 'bank', 'atm',\n",
    "                               'doctors', 'dentist', 'laundry', 'hospital',\n",
    "                               'car_wash', 'veterinary', 'hairdresser', 'optician'], \n",
    "                  'parks' : ['viewpoint', 'park', 'playground', 'picnic_site', \n",
    "                             'pitch', 'swimming_pool', 'sports_centre', \n",
    "                             'golf_course', 'track', 'dog_park'], \n",
    "                  'education' : ['college', 'school', 'kindergarten', 'university'], \n",
    "                  'entertainment' : ['library', 'attraction', 'stadium', \n",
    "                                     'arts_centre', 'theatre', 'artwork', \n",
    "                                     'archaeological', 'cinema', 'museum', \n",
    "                                     'ruins', 'observation_tower', \n",
    "                                     'community_centre', 'zoo', 'castle', \n",
    "                                     'theme_park', 'ice_rink'], \n",
    "                 'trains' : ['ferry_terminal', 'railway_station', 'bus_station', \n",
    "                             'tram_stop', 'railway_halt', 'publictransport'], \n",
    "                 'transport' : ['car_sharing', 'bus_stop']}\n",
    "\n",
    "osm_categorised = categorise_pois(osm_df, osm_categories, \n",
    "                                  category_column='fclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1bf6",
   "metadata": {},
   "source": [
    "Need to remove potential overlap between different data sources (and inside some data sources). For this dataset it's around 30% because there is overlap of public transport stops between OSM and transport agencies, and overlap of things like post offices between OSM and SSNSW. Then take this combined POI set and clip it to the study area: should be the same area as is covered by the network. This is important otherwise points outside the network may be erroneously linked to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747397b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 39.54% duplicate points from dataframes\n"
     ]
    }
   ],
   "source": [
    "pois = remove_duplicate_pois([osm_categorised, \n",
    "                              metro_categorised], buffer=10)\n",
    "\n",
    "pois = gpd.clip(pois, Greater_Adelaide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ca191",
   "metadata": {},
   "source": [
    "Choose walk index weightings, and output the sums of each category and the total to check. The walk index will be out of 100 regardless of this sum, but it is important to note that eg. shopping is only '10% of the walk index' if shopping is 10 out of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd0dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_weights = {\n",
    "    \"employment\": [10],\n",
    "    \"eating\": [3, 3, 3, 2, 2, 1, 1, 1, 1, 1],\n",
    "    \"groceries\": [10, 4],\n",
    "    \"shopping\": [2, 2, 2, 2, 2],\n",
    "    \"errands\": [6, 2, 4],\n",
    "    \"parks\": [6],\n",
    "    \"education\": [10],\n",
    "    \"entertainment\": [5],\n",
    "    \"trains\": [10],\n",
    "    \"transport\": [2.5, 2.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2440fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'employment': 10, 'eating': 18, 'groceries': 14, 'shopping': 10, 'errands': 12, 'parks': 6, 'education': 10, 'entertainment': 5, 'trains': 10, 'transport': 5.0}\n",
      "total:  100.0\n"
     ]
    }
   ],
   "source": [
    "category_sums = {k: sum(v) for k, v in poi_weights.items()}\n",
    "total = sum(category_sums.values())\n",
    "print(category_sums)\n",
    "print(\"total: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd7581",
   "metadata": {},
   "source": [
    "### Import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b6f48",
   "metadata": {},
   "source": [
    "In this case the network is already in the same projected CRS as everything else but I have left in the transformation to be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d529acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (3,4,5,7,8,9,11,16,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\z3258367\\Anaconda3\\envs\\ox\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: DtypeWarning: Columns (6,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# reading directly with geopandas.read_file crashes on my computer so I read into pandas then convert to gdf instead\n",
    "edges_df = pd.read_csv(\"adelaide_edges_5_parks.csv\")\n",
    "nodes_df = pd.read_csv(\"adelaide_nodes_5.csv\")\n",
    "edges = gpd.GeoDataFrame(edges_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(edges_df['geometry'])).set_crs(proj_crs)\n",
    "nodes = gpd.GeoDataFrame(nodes_df, \n",
    "                         geometry=gpd.GeoSeries.from_wkt(nodes_df['geometry'])).set_crs(proj_crs)\n",
    "edges = edges.to_crs(proj_crs)\n",
    "nodes = nodes.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395d9f",
   "metadata": {},
   "source": [
    "Pandana expects edges to have a two item index based on the same IDs as the node index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31dda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.set_index('connect_id',inplace=True)\n",
    "\n",
    "edges['from_idx'] = edges['from']\n",
    "edges['to_idx'] = edges['to']\n",
    "edges= edges.set_index(['from_idx', 'to_idx'])\n",
    "edges.index.names= ['from_idx','to_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea582800",
   "metadata": {},
   "source": [
    "Pandana network creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1149fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_network = pdna.Network(nodes['x'], nodes['y'],\n",
    "                                   edges['from'], edges['to'], \n",
    "                                   edges[['length']])\n",
    "\n",
    "maximum_dist = 2400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4efc6",
   "metadata": {},
   "source": [
    "Pandana network querying. The 'employment' category is empty because we didn't add the employment points to the POI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e13f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category employment is empty\n",
      "Finished category: eating\n",
      "Finished category: groceries\n",
      "Finished category: shopping\n",
      "Finished category: errands\n",
      "Finished category: parks\n",
      "Finished category: education\n",
      "Finished category: entertainment\n",
      "Finished category: trains\n",
      "Finished category: transport\n"
     ]
    }
   ],
   "source": [
    "results = walk_index(distance_network, pois, poi_weights, distance=maximum_dist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c068c0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "connect_id\n",
       "0          115.042999\n",
       "1          125.544998\n",
       "2           62.541000\n",
       "3           21.211000\n",
       "4          190.306000\n",
       "              ...    \n",
       "1813031     27.712999\n",
       "1813032      9.666000\n",
       "1813034     22.007000\n",
       "1813035     18.525000\n",
       "1813036      8.803000\n",
       "Name: parks1, Length: 1722184, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['parks1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06435bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Adelaide_colournoemployment_180222.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc063b9",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8af95",
   "metadata": {},
   "source": [
    "The current approach is to find up to 100 closest employment nodes within the maximum distance. Then look up the number of jobs at each one, apply a distance decay function to each distance, multiply these together, and sum.\n",
    "\n",
    "An alternative approach which would be more convenient would be to use the Pandana 'aggregate' function which aggregates from all nodes within the maximum distance. However, there is limited ability to change the distance decay rate within the aggregation function. It can either be flat (no decay), linear (going to 0 at the max distance), or exponential where beta is set as 1/max distance. For walking I would like a beta of 0.001, but this requires the radius to be 1000m. If the radius is 2400m, beta is only 0.0004. This can be changed in the future if the Pandana function is updated to take a decay parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "438bb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (employment_centrs['geometry'].x, employment_centrs['geometry'].y)\n",
    "\n",
    "distance_network.set_pois(category='employment', maxdist=maximum_dist, maxitems=100, x_col=x, y_col=y)\n",
    "\n",
    "employment_access = distance_network.nearest_pois(\n",
    "    distance=maximum_dist, category='employment', num_pois=100, include_poi_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c501b",
   "metadata": {},
   "source": [
    "The nearest_pois function returns both distances and the IDs of the nearest pois (with include_poi_ids option). The IDs can then be used to retrieve the number of jobs at each point. I found a merge was the fastest way to join this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c196467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itermerge(dataframe, jobs):\n",
    "    i=0\n",
    "    for column in dataframe:\n",
    "        dataframe = dataframe.merge(jobs, how='left', left_on = column, right_index = True, suffixes = [None, i])\n",
    "        i = i + 1\n",
    "    return dataframe\n",
    "\n",
    "def access_weight(x):\n",
    "        beta = -0.001\n",
    "        if x == maximum_dist:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.exp(beta*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02b8a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobcounts = itermerge(employment_access.iloc[:,100:200], employment_centrs['MB Job Count'])\n",
    "\n",
    "results['jobs'] = ((employment_access.iloc[:,0:100].applymap(access_weight))*\n",
    "                                jobcounts.iloc[:,100:200].values\n",
    "                                ).sum(axis=1)\n",
    "\n",
    "weight = 100*sum(poi_weights['employment'])/sum(sum(list(poi_weights.values()),[]))\n",
    "\n",
    "results['employment'] = weight*results['jobs']/max(results['jobs'])\n",
    "\n",
    "results['Walk_Index'] = results['Walk_Index'] + results['employment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d110b0",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfcd47",
   "metadata": {},
   "source": [
    "Filter the results to the original Colouring Sydney buildings only. Optionally export results as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec2e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results = results.filter(items=nodes[nodes['connect_type'] == 'poi'].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be4569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_results.to_csv(\"Colouring_bf_results_180221.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb824b",
   "metadata": {},
   "source": [
    "Import building footprints and join the data to them, then export these polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f943788",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = gpd.GeoDataFrame(building_results, geometry = gpd.GeoSeries.from_xy(building_results.x, building_results.y, crs=\"EPSG:7856\"))\n",
    "\n",
    "buildings_foot = gpd.read_file(folder + \"adelaide_bf.shp\").to_crs(proj_crs)\n",
    "\n",
    "# join to data\n",
    "buildings_foot = gpd.sjoin(buildings_foot, results_gdf, how='left', predicate='contains')\n",
    "\n",
    "buildings_foot.to_file(\"Colouring_bf_results_180221.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7fc10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
